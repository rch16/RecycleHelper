{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.freecodecamp.org/news/creating-your-first-image-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "from pathlib import Path\n",
    "from glob2 import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile as zf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import shutil\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D \n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract contents of zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zipfile.ZipFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "files = zf.ZipFile(\"dataset-resized.zip\",'r')\n",
    "files.extractall()\n",
    "files.close()\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to sort data into folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions ##\n",
    "\n",
    "## splits indices for a folder into train, validation, and test indices with random sampling\n",
    "    ## input: folder path\n",
    "    ## output: train, valid, and test indices    \n",
    "def split_indices(folder,seed1,seed2):    \n",
    "    n = len(os.listdir(folder))\n",
    "    full_set = list(range(1,n+1))\n",
    "\n",
    "    ## train indices\n",
    "    random.seed(seed1)\n",
    "    train = random.sample(list(range(1,n+1)),int(.5*n))\n",
    "\n",
    "    ## temp\n",
    "    remain = list(set(full_set)-set(train))\n",
    "\n",
    "    ## separate remaining into validation and test\n",
    "    random.seed(seed2)\n",
    "    valid = random.sample(remain,int(.5*len(remain)))\n",
    "    test = list(set(remain)-set(valid))\n",
    "    \n",
    "    return(train,valid,test)\n",
    "\n",
    "## gets file names for a particular type of trash, given indices\n",
    "    ## input: waste category and indices\n",
    "    ## output: file names \n",
    "def get_names(waste_type,indices):\n",
    "    file_names = [waste_type+str(i)+\".jpg\" for i in indices]\n",
    "    return(file_names)    \n",
    "\n",
    "## moves group of source files to another folder\n",
    "    ## input: list of source files and destination folder\n",
    "    ## no output\n",
    "def move_files(source_files,destination_folder):\n",
    "    for file in source_files:\n",
    "        shutil.move(file,destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure directory into train, test and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paths will be train/cardboard, train/glass, etc...\n",
    "subsets = ['train','valid']\n",
    "waste_types = ['cardboard','glass','metal','paper','plastic','trash']\n",
    "\n",
    "## create destination folders for data subset and waste type\n",
    "for subset in subsets:\n",
    "    for waste_type in waste_types:\n",
    "        folder = os.path.join('data',subset,waste_type)\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            \n",
    "if not os.path.exists(os.path.join('data','test')):\n",
    "    os.makedirs(os.path.join('data','test'))\n",
    "            \n",
    "## move files to destination folders for each waste type\n",
    "for waste_type in waste_types:\n",
    "    source_folder = os.path.join('dataset-resized',waste_type)\n",
    "    train_ind, valid_ind, test_ind = split_indices(source_folder,1,1)\n",
    "    \n",
    "    ## move source files to train\n",
    "    train_names = get_names(waste_type,train_ind)\n",
    "    train_source_files = [os.path.join(source_folder,name) for name in train_names]\n",
    "    train_dest = \"data/train/\"+waste_type\n",
    "    move_files(train_source_files,train_dest)\n",
    "    \n",
    "    ## move source files to valid\n",
    "    valid_names = get_names(waste_type,valid_ind)\n",
    "    valid_source_files = [os.path.join(source_folder,name) for name in valid_names]\n",
    "    valid_dest = \"data/valid/\"+waste_type\n",
    "    move_files(valid_source_files,valid_dest)\n",
    "    \n",
    "    ## move source files to test\n",
    "    test_names = get_names(waste_type,test_ind)\n",
    "    test_source_files = [os.path.join(source_folder,name) for name in test_names]\n",
    "    ## I use data/test here because the images can be mixed up\n",
    "    move_files(test_source_files,\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a path to the folder containing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/RebeccaHallam/OneDrive - Imperial College London/University/EE4 (19-20)/Final Year Project/App/Model/dataset-resized\n"
     ]
    }
   ],
   "source": [
    "# train_path = Path(os.getcwd())/\"data/train\"\n",
    "# test_path = Path(os.getcwd())/\"data/test\"\n",
    "# valid_path = Path(os.getcwd())/\"data/valid\"\n",
    "path = Path(os.getcwd())/\"dataset-resized\"\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/RebeccaHallam/OneDrive - Imperial College London/University/EE4 (19-20)/Final Year Project/App/Model/data/train\n",
      "/Users/RebeccaHallam/OneDrive - Imperial College London/University/EE4 (19-20)/Final Year Project/App/Model/data/test\n",
      "/Users/RebeccaHallam/OneDrive - Imperial College London/University/EE4 (19-20)/Final Year Project/App/Model/data/valid\n",
      "/Users/RebeccaHallam/OneDrive - Imperial College London/University/EE4 (19-20)/Final Year Project/App/Model/dataset-resized\n"
     ]
    }
   ],
   "source": [
    "def createFileList(myDir, format='.jpg'):\n",
    "    fileList = []\n",
    "    print(myDir)\n",
    "    for root, dirs, files in os.walk(myDir, topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith(format):\n",
    "                fullName = os.path.join(root, name)\n",
    "                fileList.append(fullName)\n",
    "    return fileList\n",
    "\n",
    "fileList_train = createFileList(train_path,format='.jpg')\n",
    "fileList_test = createFileList(test_path,format='.jpg')\n",
    "fileList_valid = createFileList(valid_path,format='.jpg')\n",
    "fileList = createFileList(path,format='.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262\n",
      "635\n",
      "630\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(fileList_train))\n",
    "print(len(fileList_test))\n",
    "print(len(fileList_valid))\n",
    "print(len(fileList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    1: \"paper\",\n",
    "    2: \"cardboard\",\n",
    "    3: \"trash\",\n",
    "    4: \"plastic\",\n",
    "    5: \"metal\",\n",
    "    6: \"glass\",\n",
    "    7: \"unknown\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to images to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262, 196608)\n",
      "(635, 196608)\n",
      "(630, 196608)\n"
     ]
    }
   ],
   "source": [
    "def stringContains(filePath):\n",
    "    if \"paper\" in filePath:\n",
    "        return 1\n",
    "    elif \"cardboard\" in filePath:\n",
    "        return 2\n",
    "    elif \"trash\" in filePath:\n",
    "        return 3\n",
    "    elif \"plastic\" in filePath:\n",
    "        return 4\n",
    "    elif \"metal\" in filePath:\n",
    "        return 5\n",
    "    elif \"glass\" in filePath:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def convertToMatrix(fileList,num_images):\n",
    "    \n",
    "    num_pixels = 196608\n",
    "    data = np.empty((num_images,num_pixels))\n",
    "    labels = []\n",
    "    index = 0\n",
    "    \n",
    "    for file in fileList:\n",
    "        \n",
    "        img = Image.open(file) # open image\n",
    "        arr = np.array(img.convert('L')) # convert to grey and place in numpy array\n",
    "        vec = arr.ravel() # flatten to vector\n",
    "        data[index,:] = vec.T # add to matrix\n",
    "        \n",
    "        name = stringContains(img.filename) # get image name\n",
    "        labels = np.append(labels,name) # add to array\n",
    "        \n",
    "        index += 1\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "train,train_l = convertToMatrix(fileList_train,1262)\n",
    "test,test_l = convertToMatrix(fileList_test,635)\n",
    "valid,valid_l = convertToMatrix(fileList_valid,630)\n",
    "\n",
    "train = train.astype('float32')\n",
    "train_l = train_l.astype(int)\n",
    "test = test.astype('float32')\n",
    "test_l = test_l.astype(int)\n",
    "valid = valid.astype('float32')\n",
    "valid_l = valid_l.astype(int)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert categories (labels) into one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262, 7)\n",
      "(635, 7)\n",
      "(630, 7)\n"
     ]
    }
   ],
   "source": [
    "train_labels = to_categorical(train_l)\n",
    "test_labels = to_categorical(test_l)\n",
    "valid_labels = to_categorical(valid_l)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 384\n",
    "cols = 512\n",
    "input_shape = (rows,cols,1)\n",
    "\n",
    "train_data = train.reshape(train.shape[0],rows,cols,1)\n",
    "test_data = test.reshape(test.shape[0],rows,cols,1)\n",
    "valid_data = valid.reshape(valid.shape[0],rows,cols,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalise image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data /= 255.0\n",
    "test_data /= 255.0\n",
    "valid_data /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 380, 508, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 190, 254, 32)      0         \n",
      "=================================================================\n",
      "Total params: 832\n",
      "Trainable params: 832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 32 layers, 5x5 convolutional layer window, 2x2 pooling layer window\n",
    "model.add(Conv2D(32,(5,5),activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 380, 508, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 190, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 186, 250, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 93, 125, 64)       0         \n",
      "=================================================================\n",
      "Total params: 52,096\n",
      "Trainable params: 52,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# num_classes = 7\n",
    "# x = tensorflow.placeholder(\"float\",[None,rows,cols,1])\n",
    "# y = tensorflow.placeholder(\"float\",[None,num_classes])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(5,5),activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 380, 508, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 190, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 186, 250, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 93, 125, 64)       0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 93, 125, 7)        455       \n",
      "=================================================================\n",
      "Total params: 52,551\n",
      "Trainable params: 52,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# add a densely connected layer\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (1262, 7) was passed for an output of shape (None, 93, 125, 7) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-2201da928ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(train_data, train_labels,\n\u001b[1;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1262\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           epochs=5)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (1262, 7) was passed for an output of shape (None, 93, 125, 7) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels,\n",
    "          batch_size=1262,\n",
    "          epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
